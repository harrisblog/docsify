![](https://npm.elemecdn.com/hassan-assets/posts/Computer_Network/image-20200603182006922.png)

## 一、运输层协议概述

### 1. 进程之间的通信

- 从通信和信息处理的角度看，运输层向它上面的应用层提供通信服务，<u>它属于面向通信部分的最高层，同时也是用户功能中的最低层</u>。

- 当网络的边缘部分中的两个主机使用网络的核心部分的功能进行端到端的通信时，<u>只有位于网络边缘部分的主机的协议栈才有运输层</u>，而网络核心部分中的路由器在转发分组时都只用到下三层的功能。

#### (1) 运输层的作用

- “<u>逻辑通信</u>”的意思是“好像是这样通信，但事实上并非真的这样通信”。

- <u>从 IP 层来说，通信的两端是两台主机</u>。但“两台主机之间的通信”这种说法还不够清楚。

- 严格地讲，两台主机进行通信就是两台主机中的应用进程互相通信。

- <u>从运输层的角度看，通信的真正端点并不是主机而是主机中的进程</u>。也就是说，端到端的通信是应用进程之间的通信。

#### (2) 端系统之间通信的含义

“主机 A 和主机 B 进行通信” 实际上是指：“<u>运行在主机 A 上的某个程序和运行在主机 B 上的另一个程序进行通信</u>”。端到端的通信是进程之间的通信。

#### (3) 网络层和运输层的明显区别

**网络层**是为主机之间提供逻辑通信。

**运输层**为应用进程之间提供端到端的逻辑通信。

#### (4) 屏蔽作用

运输层向高层用户<u>屏蔽</u>了下面网络核心的细节，它使应用进程看见的就是好像在两个运输层实体之间有一条<u>端到端的逻辑通信信道</u>。

#### (5) 两种不同的运输协议

- 但这条逻辑通信信道对上层的表现却因运输层使用的不同协议而有很大的差别。

- 当运输层采用面向连接的 **TCP** 协议时，尽管下面的网络是不可靠的（只提供尽最大努力服务），但这种逻辑通信信道就相当于一条<u>全双工的可靠信道</u>。

- 当运输层采用无连接的 **UDP** 协议时，这种逻辑通信信道是一条<u>不可靠信道</u>。

### 2. 运输层的两个主要协议

TCP/IP 的运输层有两个主要协议：

1. 用户数据报协议 **UDP** (User Datagram Protocol)

2. 传输控制协议 **TCP** (Transmission Control Protocol)

#### (1) UDP 与 TCP

- 两个对等运输实体在通信时传送的数据单位叫作**运输协议数据单元** **TPDU** (Transport Protocol Data Unit)。

- TCP 传送的数据单位协议是 **TCP 报文段**。

- UDP 传送的数据单位协议是 **UDP 报文**或**用户数据报**。

| UDP                                                     | TCP                                                      |
| ------------------------------------------------------- | -------------------------------------------------------- |
| 无连接的协议，提供无连接服务；                          | 面向连接的协议，提供面向连接服务；                       |
| 其传送的运输协议数据单元 TPDU 是 UDP 报文或用户数据报； | 其传送的运输协议数据单元 TPDU 是 TCP 报文；              |
| 支持单播、多播、广播；                                  | 支持点对点单播，不支持多播、广播；                       |
| 不提供可靠交付；                                        | 提供可靠服务；                                           |
| 简单。适用于很多应用，如：多媒体应用等。                | 复杂。用于大多数应用，如：万维网、电子邮件、文件传送等。 |

### 3. 运输层的端口

- 运行在计算机中的进程是用<u>进程标识符</u>来标志的。

- <u>但运行在应用层的各种应用进程却不应当让计算机操作系统指派它的进程标识符</u>。这是因为在互联网上使用的计算机的操作系统种类很多，而不同的操作系统又使用不同格式的进程标识符。

- 为了使运行不同操作系统的计算机的应用进程能够互相通信，就<u>必须用统一的方法</u>对 TCP/IP 体系的应用进程进行标志。

#### (1) 需要解决的问题

由于进程的创建和撤销都是动态的，发送方几乎无法识别其他机器上的进程。有时我们会改换接收报文的进程，但并不需要通知所有发送方。我们往往需要利用目的主机提供的功能来识别终点，而不需要知道实现这个功能的进程。

#### (2) 端口号

- 解决这个问题的方法就是在运输层使用**协议端口号** (protocol port number)，或通常简称为**端口** (port)。

- 虽然通信的终点是应用进程，但我们可以把端口想象是通信的终点，因为我们只要把要传送的报文交到目的主机的某一个合适的目的端口，剩下的工作（即最后交付目的进程）就由 TCP 来完成。

#### (3) 软件端口与硬件端口

- 两个不同的概念。

- 在协议栈层间的抽象的协议端口是<u>软件端口</u>。

- 路由器或交换机上的端口是<u>硬件端口</u>。

- 硬件端口是不同硬件设备进行交互的接口，而软件端口是应用层的各种协议进程与运输实体进行层间交互的一种地址。

#### (4) TCP/IP 运输层端口

- 端口用一个 16 位端口号进行标志，允许有 65,535 个不同的端口号。

- 端口号只具有<u>本地意义</u>，即端口号只是为了标志<u>本计算机应用层中的各进程</u>。在互联网中，不同计算机的相同端口号是没有联系的。

#### (5) 两大类端口

- <u>服务器端使用的端口号</u>

1. **熟知端口**，数值一般为 0 ~ 1023。

2. **登记端口号**，数值为 1024 ~ 49151，为没有熟知端口号的应用程序使用的。使用这个范围的端口号必须在 IANA 登记，以防止重复。

- <u>客户端使用的端口号</u>

1. 又称为**短暂端口号**，数值为 49152 ~ 65535，留给客户进程选择暂时使用。

2. 当服务器进程收到客户进程的报文时，就知道了客户进程所使用的动态端口号。通信结束后，这个端口号可供其他客户进程以后使用。

## 二、用户数据报协议 UDP

### 1. UDP 概述

- UDP 只在 IP 的数据报服务之上增加了很少一点的功能：

1. 复用和分用的功能

2. 差错检测的功能

#### (1) UDP 的主要特点

1. <u>UDP 是无连接的</u>，发送数据之前不需要建立连接，因此减少了开销和发送数据之前的时延。

2. <u>UDP 使用尽最大努力交付</u>，即不保证可靠交付，因此主机不需要维持复杂的连接状态表。

3. <u>UDP 是面向报文的</u>。UDP 对应用层交下来的报文，既不合并，也不拆分，而是保留这些报文的边界。UDP 一次交付一个完整的报文。

4. <u>UDP 没有拥塞控制</u>，因此网络出现的拥塞不会使源主机的发送速率降低。这对某些实时应用是很重要的。很适合多媒体通信的要求。

5. <u>UDP 支持一对一、一对多、多对一和多对多的交互通信</u>。

6. <u>UDP 的首部开销小</u>，只有 8 个字节，比 TCP 的 20 个字节的首部要短。

#### (2) 面向报文的 UDP

- 发送方 UDP 对应用程序交下来的报文，在添加首部后就向下交付 IP 层。UDP 对应用层交下来的报文，既不合并，也不拆分，而是保留这些报文的边界。

- 应用层交给 UDP 多长的报文，UDP 就照样发送，即一次发送一个报文。

- 接收方 UDP 对 IP 层交上来的 UDP 用户数据报，在去除首部后就原封不动地交付上层的应用进程，<u>一次交付一个完整的报文</u>。

- 应用程序必须<u>选择合适大小的报文</u>。

1. <u>若报文太长</u>，UDP 把它交给 IP 层后，IP 层在传送时可能要进行分片，这会降低 IP 层的效率。

2. <u>若报文太短</u>，UDP 把它交给 IP 层后，会使 IP 数据报的首部的相对长度太大，这也降低了 IP 层的效率。

### 2. UDP 的首部格式

用户数据报 UDP 有两个字段：数据字段和首部字段。

首部字段有 8 个字节，由 4 个字段组成，每个字段都是 2 个字节。

#### (1) UDP 基于端口的分用

当运输层从 IP 层收到 UDP 数据报时，就根据首部中的目的端口，把 UDP 数据报通过相应的端口，上交给最后的终点——应用进程。

## 三、传输控制协议 TCP 概述

### 1. TCP 最主要的特点

- TCP 是<u>面向连接</u>的运输层协议，在无连接的、不可靠的 IP 网络服务基础之上提供<u>可靠交付</u>的服务。为此，在 IP 的数据报服务基础之上，增加了保证可靠性的一系列措施。

- TCP 是<u>面向连接</u>的运输层协议。

- 每一条 TCP 连接<u>只能有两个端点</u>，每一条 TCP 连接<u>只能是点对点</u>的（一对一）。

- TCP 提供<u>可靠交付</u>的服务。

- TCP 提供<u>全双工</u>通信。

- <u>面向字节流</u>

1. TCP 中的“流”(stream) 指的是流入或流出进程的字节序列。

2. “面向字节流”的含义是：虽然应用程序和 TCP 的交互是一次一个数据块，但 TCP 把应用程序交下来的数据看成仅仅是一连串无结构的字节流。

#### (1) TCP 面向流的概念

- TCP <u>不保证</u>接收方应用程序所收到的数据块和发送方应用程序所发出的<u>数据块具有对应大小的关系</u>。

- 但接收方应用程序收到的字节流必须和发送方应用程序发出的<u>字节流完全一样</u>。

#### (2) 注意

- TCP 连接是一条<u>虚连接</u>而不是一条真正的物理连接。

- TCP 对应用进程一次把多长的报文发送到 TCP 的缓存中是不关心的。

- TCP 根据对方给出的<u>窗口值</u>和<u>当前网络拥塞</u>的程度来决定一个报文段应包含多少个字节（UDP 发送的报文长度是应用进程给出的）。

- TCP 可把太长的数据块划分短一些再传送。

- TCP 也可等待积累有足够多的字节后再构成报文段发送出去。

### 2. TCP 的连接

- TCP 把连接作为<u>最基本的抽象</u>。

- 每一条 TCP 连接有<u>两个端点</u>。

- TCP 连接的端点不是主机，不是主机的 IP 地址，不是应用进程，也不是运输层的协议端口。<u>TCP 连接的端点叫做套接字 (socket) 或插口</u>。

- <u>端口号拼接到 (contatenated with) IP 地址即构成了套接字</u>。

## 四、可靠传输的工作原理

IP 网络所提供的是不可靠的传输

**理想的传输条件特点**：

- 理想的传输条件有以下<u>两个特点</u>：

1. 传输信道不产生差错。

2. 不管发送方以多快的速度发送数据，接收方总是来得及处理收到的数据。

- 在这样的理想传输条件下，不需要采取任何措施就能够实现可靠传输。

- <u>然而实际的网络都不具备以上两个理想条件</u>。必须使用一些可靠传输协议，在不可靠的传输信道实现可靠传输。

### 1. 停止等待协议

- “停止等待”就是每发送完一个分组就停止发送，等待对方的确认。在收到确认后再发送下一个分组。

- <u>全双工通信的双方既是发送方也是接收方</u>。

- 为了讨论问题的方便，我们仅考虑 A 发送数据，而 B 接收数据并发送确认。因此 A 叫做<u>发送方</u>，而 B 叫做<u>接收方</u>。

#### (1) 无差错情况

#### (2) 出现差错

- 在接收方 B 会出现两种情况：

1. <u>B 接收 M1 时检测出了差错</u>，就<u>丢弃</u> M1，其他什么也不做（不通知 A 收到有差错的分组）。

2. <u>M1 在传输过程中丢失了</u>，这时 B 当然什么都不知道，也什么都不做。

- 在这两种情况下，<u>B 都不会发送任何信息</u>。

- 但<u>A 都必须重发分组</u>，直到 B 正确接收为止，这样才能实现可靠通信。

- **问题**：A 如何知道 B 是否正确收到了 M1 呢？

- **解决方法**：超时重传

1. A 为每一个已发送的分组都设置了一个<u>超时计时器</u>。

2. A 只要在超时计时器到期之前收到了相应的确认，就撤销该超时计时器，继续发送下一个分组 M2 。

3. 若 A 在超时计时器规定时间内没有收到 B 的确认，就认为分组错误或丢失，就重发该分组。

- **问题**：若分组正确到达 B，但 B 回送的确认丢失或延迟了，A 未收到 B 的确认，会超时重发。B 可能会收到重复的 M1 。B 如何知道收到了重复的分组，需要丢弃呢？

- **解决方法**：编号

- A 为每一个发送的分组都进行编号。若 B 收到了编号相同的分组，则认为收到了重复分组，丢弃重复的分组，并回送确认。

- B 为发送的确认也进行编号，指示该确认是对哪一个分组的确认。

- A 根据确认及其编号，可以确定它是对哪一个分组的确认，避免重发发送。若为重复的确认，则将其丢弃。

#### (3) 确认丢失和确认迟到

- **确认丢失**

1. 若 B 所发送的对 M1 的确认丢失了，那么 A 在设定的超时重传时间内不能收到确认，但 A 并无法知道：是自己发送的分组出错、丢失了，<u>或者</u> 是 B 发送的确认丢失了。因此 <u>A 在超时计时器到期后就要重传 M1</u>。

2. 假定 B 又收到了重传的分组 M1。这时 B 应采取两个行动：

   - 第一，<u>丢弃</u>这个重复的分组 M1，不向上层交付。

   - 第二，<u>向 A 发送确认</u>。不能认为已经发送过确认就不再发送，因为 A 之所以重传 M1 就表示 A 没有收到对 M1 的确认。

- **确认迟到**

1. 传输过程中没有出现差错，但 B 对分组 M1 的确认迟到了。

2. A 会收到重复的确认。对重复的确认的处理很简单：收下后就丢弃。

3. B 仍然会收到重复的 M1，并且同样要丢弃重复的 M1，并重传确认分组。

##### A. 请注意

- 在发送完一个分组后，必须<u>暂时保留</u>已发送的分组的副本，以备重发。

- <u>分组和确认分组都必须进行编号</u>。

- 超时计时器的重传时间应当比数据在分组传输的平均往返时间<u>更长一些</u>。

##### B. 自动重传请求 ARQ

- <u>通常 A 最终总是可以收到对所有发出的分组的确认</u>。如果 A 不断重传分组但总是收不到确认，就说明通信线路太差，不能进行通信。

- <u>使用上述的确认和重传机制，我们就可以在不可靠的传输网络上实现可靠的通信</u>。

- 像上述的这种可靠传输协议常称为**自动重传请求 ARQ** (Automatic Repeat reQuest)。意思是重传的请求是自动进行的，接收方不需要请求发送方重传某个出错的分组。

#### (4) 信道利用率

- 可以看出，当往返时间 RTT 远大于分组发送时间 _T~D~_ 时，信道的利用率就会非常低。

- 若出现重传，则对传送有用的数据信息来说，信道的利用率就还要降低。

##### A. 流水线传输

- 为了提高传输效率，发送方可以不使用低效率的停止等待协议，而是采用流水线传输。

- <u>流水线传输</u>就是发送方可连续发送多个分组，不必每发完一个分组就停顿下来等待对方的确认。这样可使信道上一直有数据不间断地传送。

- 由于信道上一直有数据不间断地传送，这种传输方式可获得很高的信道利用率。

### 2. 连续 ARQ 协议

**基本思想**：

- 发送方一次可以发出<u>多个分组</u>。

- 使用<u>滑动窗口协议</u>控制发送方和接收方所能发送和接收的分组的数量和编号。

- 每收到一个确认，发送方就把发送窗口<u>向前滑动</u>。

- 接收方一般采用<u>累积确认</u>的方式。

- 采用**回退 N**（Go-Back-N）方法进行重传。

- 滑动窗口协议比较复杂，是 TCP 协议的精髓所在。

- 发送方维持的<u>发送窗口</u>，它的意义是：<u>位于发送窗口内的分组都可连续发送出去，而不需要等待对方的确认</u>。这样，信道利用率就提高了。

- 连续 ARQ 协议规定，<u>发送方每收到一个确认，就把发送窗口向前滑动一个分组的位置</u>。

#### (1) 累积确认

- 接收方一般采用<u>累积确认</u>的方式。即不必对收到的分组逐个发送确认，而是<u>对按序到达的最后一个分组发送确认</u>，这样就表示：<u>到这个分组为止的所有分组都已正确收到了</u>。

- **优点**：容易实现，即使确认丢失也不必重传。

- **缺点**：不能向发送方反映出接收方已经正确收到的所有分组的信息。

#### (2) Go-back-N（回退 N）

- 如果发送方发送了前 5 个分组，而中间的第 3 个分组丢失了。这时接收方只能对前两个分组发出确认。发送方无法知道后面三个分组的下落，<u>而只好把后面的三个分组都再重传一次</u>。

- 这就叫做 Go-back-N（<u>回退 N</u>），<u>表示需要再退回来重传已发送过的 N 个分组</u>。

- 可见当通信线路质量不好时，连续 ARQ 协议会带来负面的影响。

#### (3) TCP 可靠通信的具体实现

- TCP 连接的每一端都必须设有两个窗口——一个<u>发送窗口</u>和一个<u>接收窗口</u>。

- TCP 的可靠传输机制用<u>字节的序号</u>进行控制。TCP 所有的确认都是基于序号而不是基于报文段。

- TCP 两端的四个窗口经常处于<u>动态变化</u>之中。

- TCP 连接的往返时间 RTT 也不是固定不变的。需要使用特定的算法<u>估算较为合理的重传时间</u>。

#### (4) 连续 ARQ 协议与停止等待协议

|                | 连续 ARQ 协议       | 停止等待协议     |
| -------------- | ------------------- | ---------------- |
| 发送的分组数量 | 一次发送多个分组    | 一次发送一个分组 |
| 传输控制       | 滑动窗口协议        | 停等-等待        |
| 确认           | 单独确认 + 累积确认 | 单独确认         |
| 超时定时器     | 每个发送的分组      | 每个发送的分组   |
| 编号           | 每个发送的分组      | 每个发送的分组   |
| 重传           | 回退 N，多个分组    | 一个分组         |

## 五、TCP 报文段的首部格式

- TCP 虽然是面向字节流的，但 TCP 传送的数据单元却是报文段。

- 一个 TCP 报文段分为首部和数据两部分，而 TCP 的全部功能都体现在它首部中各字段的作用。

- TCP 报文段首部的前 20 个字节是固定的，后面有 4*n* 字节是根据需要而增加的选项 (_n_ 是整数)。<u>因此 TCP 首部的最小长度是 20 字节</u>。

**MSS** (Maximum Segment Size)是 TCP 报文段中的数据字段的最大长度。数据字段加上 TCP 首部才等于整个的 TCP 报文段。所以，MSS 是“TCP 报文段长度减去 TCP 首部长度”。

### 1. 为什么要规定 MSS ？

- <u>MSS 与接收窗口值没有关系</u>。

- 若<u>选择较小的 MSS 长度，网络的利用率就降低</u>。

- 若 <u>TCP 报文段非常长</u>，那么在 IP 层传输时就有可能要分解成多个短数据报片。在终点要把收到的各个短数据报片装配成原来的 TCP 报文段。当传输出错时还要进行重传。这些也都会<u>使开销增大</u>。

- 因此，MSS 应尽可能大些，只要在 IP 层传输时不需要再分片就行。

- 但最佳的 MSS 是很难确定的。

### 2. 其他选项

- **窗口扩大选项** ——占 3 字节，其中有一个字节表示移位值 S。新的窗口值等于 TCP 首部中的窗口位数增大到 (16 + S)，相当于把窗口值向左移动 S 位后获得实际的窗口大小。

- **时间戳选项**——占 10 字节，其中最主要的字段时间戳值字段（4 字节）和时间戳回送回答字段（4 字节）。

- **选择确认选项**——在后面的 5.6.3 节介绍。

## 六、TCP 可靠传输的实现

### 1. 以字节为单位的滑动窗口

- TCP 使用流水线传输和滑动窗口协议实现高效、可靠的传输。

- TCP 的滑动窗口是<u>以字节为单位</u>的。

- 发送方 A 和接收方 B 分别维持一个发送窗口和一个接收窗口。

- **发送窗口表示**：在没有收到确认的情况下，可以连续把窗口内的数据全部发送出去。

- **接收窗口表示**：只允许接收落入窗口内的数据。

#### (1) 发送缓存与接收缓存的作用

- <u>发送缓存</u>用来暂时存放：

1. <u>发送应用程序传送给发送方 TCP 准备发送的数据</u>；

2. <u>TCP 已发送出但尚未收到确认的数据</u>。

- <u>接收缓存</u>用来暂时存放：

1. <u>按序到达的、但尚未被接收应用程序读取的数据</u>；

2. <u>不按序到达的数据</u>。

#### (2) 需要强调三点

- 第一，A 的发送窗口并<u>不总是</u>和 B 的接收窗口一样大（因为有一定的时间滞后）。

- 第二，TCP 标准<u>没有规定</u>对不按序到达的数据应如何处理。通常是先临时存放在接收窗口中，等到字节流中所缺少的字节收到后，再按序交付上层的应用进程。

- 第三，TCP 要求接收方必须有<u>累积确认</u>的功能，这样可以减小传输开销。

#### (3) 接收方发送确认

- 接收方可以在<u>合适的时候发送确认</u>，也可以在自己有数据要发送时把确认信息<u>顺便捎带上</u>。

- 但请注意两点：

1. 第一，接收方不应过分推迟发送确认，否则会导致发送方不必要的重传，这反而浪费了网络的资源。。

2. 第二，捎带确认实际上并不经常发生，因为大多数应用程序很少同时在两个方向上发送数据。

### 2. 超时重传时间的选择

- 重传机制是 TCP 中最重要和最复杂的问题之一。

- TCP 每发送一个报文段，就对这个报文段设置一次计时器。

- 只要计时器设置的重传时间到但还没有收到确认，就要重传这一报文段。

- <u>重传时间的选择是 TCP 最复杂的问题之一</u>。

#### (1) 往返时延的方差很大

由于 TCP 的下层是一个互联网环境，IP 数据报所选择的路由变化很大。因而运输层的往返时间 (RTT) 的方差也很大。

#### (2) TCP 超时重传时间设置

- 如果把超时重传时间设置得太短，就会引起很多报文段的不必要的重传，使网络负荷增大。

- 但若把超时重传时间设置得过长，则又使网络的空闲时间增大，降低了传输效率。

- <u>TCP 采用了一种自适应算法</u>，它记录一个报文段发出的时间，以及收到相应的确认的时间。这两个时间之差就是报文段的往返时间 RTT。

#### (3) 加权平均往返时间

- TCP 保留了 RTT 的一个**加权平均往返时间**RTT~S~（这又称为<u>平滑的往返时间</u>）。

- 第一次测量到 RTT 样本时，RTT~S~ 值就取为所测量到的 RTT 样本值。以后每测量到一个新的 RTT 样本，就按下式重新计算一次 RTT~S~：

$$
新的RTTS = (1 - α)×(旧的RTTS)  + α×(新的RTT样本)
$$

- 式中，0 ≤ α < 1。若 α 很接近于零，表示 RTT 值更新较慢。若选择 α 接近于 1，则表示 RTT 值更新较快。

- RFC 6298 推荐的 α 值为 1/8，即 0.125。

#### (4) 超时重传时间 RTO

- <u>RTO (Retransmission Time-Out) 应略大于上面得出的加权平均往返时间 RTT~S~</u>。

- RFC 6298 建议使用下式计算 RTO：

$$
RTO = RTTS + 4×RTTD
$$

- RTTD 是 <u>RTT 的偏差的加权平均值</u>。

- RFC 6298 建议这样计算 RTT~D~ 。第一次测量时， RTT~D~ 值取为测量到的 RTT 样本值的一半。在以后的测量中，则使用下式计算加权平均的 RTT~D~ ：

$$
新的 RTTD = (1-β)×(旧的RTTD) + β×|RTTS-新的 RTT 样本|
$$

- b 是个小于 1 的系数，其推荐值是 1/4，即 0.25。

#### (5) 往返时间 (RTT) 的测量相当复杂

- TCP 报文段 1 没有收到确认。重传（即报文段 2）后，收到了确认报文段 ACK。

- <u>如何判定此确认报文段是对原来的报文段 1 的确认，还是对重传的报文段 2 的确认</u>？

#### (6) Karn 算法

- <u>在计算平均往返时间 RTT 时，只要报文段重传了，就不采用其往返时间样本</u>。

- 这样得出的加权平均平均往返时间 RTTS 和超时重传时间 RTO 就较准确。

- 但是，这又引起<u>新的问题</u>。当报文段的时延突然增大了很多时，在原来得出的重传时间内，不会收到确认报文段。于是就重传报文段。但根据 Karn 算法，不考虑重传的报文段的往返时间样本。这样，<u>超时重传时间就无法更新</u>。

#### (7) 修正的 Karn 算法

- 报文段每重传一次，就把 RTO 增大一些：

$$
新的RTO= γ × (旧的RTO)
$$

- 系数 g 的典型值是 2 。

- 当不再发生报文段的重传时，才根据报文段的往返时延更新平均往返时延 RTT 和超时重传时间 RTO 的数值。

- 实践证明，这种策略较为合理。

### 3. 选择确认 SACK

- **问题**：若收到的报文段无差错，只是未按序号，中间还缺少一些序号的数据，那么能否设法只传送缺少的数据而不重传已经正确到达接收方的数据？

- 答案是可以的。<u>选择确认 SACK</u> (Selective ACK) 就是一种可行的处理方法。

## 七、TCP 的流量控制

### 1. 利用滑动窗口实现流量控制

- 一般说来，我们总是希望数据传输得更快一些。但如果发送方把数据发送得过快，接收方就可能来不及接收，这就会造成数据的丢失。

- **流量控制** (flow control) 就是让发送方的发送速率不要太快，既要让接收方来得及接收，也不要使网络发生拥塞。

- 利用<u>滑动窗口机制</u>可以很方便地在 TCP 连接上实现流量控制。

#### (1) 可能发生死锁

- B 向 A 发送了零窗口的报文段后不久，B 的接收缓存又有了一些存储空间。于是 B 向 A 发送了 rwnd = 400 的报文段。

- 但这个报文段在传送过程中<u>丢失</u>了。A 一直等待收到 B 发送的非零窗口的通知，而 B 也一直等待 A 发送的数据。

- 如果没有其他措施，这种<u>互相等待的死锁</u>局面将一直延续下去。

- 为了解决这个问题，TCP 为每一个连接设有一个**持续计时器** (persistence timer)。

#### (2) 持续计时器

- 为了解决这个问题， TCP 为每一个连接设有一个**持续计时器** (persistence timer) 。

- 只要 TCP 连接的一方收到对方的<u>零窗口</u>通知，就启动该持续计时器。

- 若持续计时器设置的时间到期，就发送一个<u>零窗口探测报文段</u>（仅携带 1 字节的数据），而对方就在确认这个探测报文段时给出了现在的窗口值。

- 若窗口仍然是零，则收到这个报文段的一方就重新设置持续计时器。

- 若窗口不是零，则死锁的僵局就可以打破了。

### 2. TCP 的传输效率

- 可以用不同的机制来控制 TCP 报文段的发送时机:

1. **第一种机制**是 TCP 维持一个变量，它等于最大报文段长度 MSS。只要缓存中存放的数据达到 MSS 字节时，就组装成一个 TCP 报文段发送出去。

2. **第二种机制**是由发送方的应用进程指明要求发送报文段，即 TCP 支持的推送 (push) 操作。

3. **第三种机制**是发送方的一个计时器期限到了，这时就把当前已有的缓存数据装入报文段（但长度不能超过 MSS）发送出去。

- 如何控制 TCP 发送报文段的时机仍然是一个较为复杂的问题。

#### (1) 糊涂窗口综合症

- **糊涂窗口综合症**：每次仅发送一个字节或很少几个字节的数据时，有效数据传输效率变得很低的现象。

#### (2) 发送方糊涂窗口综合症

- 发送方 TCP <u>每次接收到一字节</u>的数据后就发送。

- 这样，发送一个字节需要形成 41 字节长的 IP 数据报。效率很低。

- **解决方法**：使用 Nagle 算法。

#### (3) Nagle 算法

- 若发送应用进程把要发送的数据逐个字节地送到 TCP 的发送缓存，则发送方就把第一个数据字节先发送出去，把后面到达的数据字节都缓存起来。

- 当发送方收到对第一个数据字符的确认后，再把发送缓存中的所有数据组装成一个报文段发送出去，同时继续对随后到达的数据进行缓存。

- 只有在收到对前一个报文段的确认后才继续发送下一个报文段。

- 当到达的数据已达到发送窗口大小的一半或已达到报文段的最大长度时，就立即发送一个报文段。

#### (4) 接收方糊涂窗口综合症

- 当接收方的 TCP 缓冲区已满，接收方会向发送方发送窗口大小为 0 的报文。

- 若此时接收方的应用进程以交互方式每次只读取一个字节，于是接收方又发送窗口大小为一个字节的更新报文，发送方应邀发送一个字节的数据（发送的 IP 数据报是 41 字节长），于是接收窗口又满了，如此循环往复。

- **解决方法**：让接收方等待一段时间，使得或者接收缓存已有足够空间容纳一个最长的报文段，或者等到接收缓存已有一半空闲的空间。<u>只要出现这两种情况之一，接收方就发出确认报文，并向发送方通知当前的窗口大小</u>。

## 八、TCP 的拥塞控制

### 1. 拥塞控制的一般原理

- 在某段时间，若对网络中某资源的需求超过了该资源所能提供的可用部分，网络的性能就要变坏。这种现象称为**拥塞** (congestion)。

- 最坏结果：<u>系统崩溃</u>。

#### (1) 拥塞产生的原因

- 网络拥塞往往是由许多因素引起的。例如：

1. 点缓存的容量太小；

2. 链路的容量不足；

3. 处理机处理的速率太慢；

4. 拥塞本身会进一步加剧拥塞；

- 出现拥塞的原因：

$$
∑ 对资源需求 > 可用资源
$$

#### (2) 增加资源能解决拥塞吗？

- <u>不能</u>。这是因为网络拥塞是一个非常复杂的问题。简单地采用上述做法，在许多情况下，不但不能解决拥塞问题，而且还可能使网络的性能更坏。

- 网络拥塞往往是由许多因素引起的。例如：

1. 增大缓存，但未提高输出链路的容量和处理机的速度，排队等待时间将会大大增加，引起大量超时重传，解决不了网络拥塞；

2. 提高处理机处理的速率会会将瓶颈转移到其他地方；

#### (3) 拥塞控制与流量控制的区别

| 拥塞控制                                                            | 流量控制                                                |
| ------------------------------------------------------------------- | ------------------------------------------------------- |
| 防止过多的数据注入到网络中，使网络中的路由器或链路<u>不致过载</u>； | 抑制发送端发送数据的速率，以使接收端<u>来得及接收</u>； |
| 是一个<u>全局性</u>的过程，涉及到与降低网络传输性能有关的所有因素。 | 是点对点通信量的控制，是<u>端到端</u>的问题；           |

#### (4) 拥塞控制的一般原理

- 拥塞控制的前提：网络能够承受现有的网络负荷。

- 实践证明，拥塞控制是很难设计的，因为它是一个<u>动态问题</u>。

- 分组的丢失是网络发生拥塞的<u>征兆</u>而不是原因。

- 在许多情况下，甚至正是<u>拥塞控制本身</u>成为引起网络性能恶化、甚至发生死锁的原因。

#### (5) 开环控制和闭环控制

| 开环控制                                           | 闭环控制                                                     |
| -------------------------------------------------- | ------------------------------------------------------------ |
| 在设计网络时，事先考虑周全，力求工作时不发生拥塞； | 基于反馈环路的概念；根据网络当前的运行状态采取相应控制措施； |
| **思路**：力争避免发生拥塞。                       | **思路**：在发生拥塞后，采取措施进行控制，消除拥塞。         |

#### (6) 闭环控制

- 属于闭环控制的有以下几种措施：

(1) 监测网络系统，以便检测到拥塞在何时、何处发生。

(2) 将拥塞发生的信息传送到可采取行动的地方。

(3) 调整网络系统的运行以解决出现的问题。

#### (6) 监测网络的拥塞

- 主要指标有：

1. 由于缺少缓存空间而被丢弃的分组的百分数；

2. 平均队列长度；

3. 超时重传的分组数；

4. 平均分组时延；

5. 分组时延的标准差，等等。

- 上述这些指标的上升都标志着拥塞的增长。

#### (7) 传递拥塞通知

- 发送通知拥塞发生的分组；

- 在分组中保留表示拥塞状态的字段；

- 周期性地发出探测分组等。

#### (8) 采取行动的时机

- 过于频繁，会使系统产生不稳定的振荡；

- 过于迟缓地采取行动又不具有任何实用价值。

#### (9) 解决拥塞的两条思路

- 增加网络可用资源；

- 减少用户对资源的需求。

### 2. TCP 的拥塞控制方法

- TCP 采用<u>基于窗口的方法</u>进行拥塞控制。该方法属于闭环控制方法。

- TCP 发送方维持一个**拥塞窗口 cwnd** (Congestion Window)

- 发送端利用<u>拥塞窗口</u>根据网络的拥塞情况调整发送的数据量。

- 发送窗口大小不仅取决于接收方窗口，还取决于网络的拥塞状况，所以<u>真正的发送窗口值</u>为：

$$
真正的发送窗口值 = Min (接收方窗口值，拥塞窗口值)
$$

#### (1) 控制拥塞窗口的原则

- 只要网络没有出现拥塞，拥塞窗口就可以再增大一些，以便把更多的分组发送出去，这样就可以提高网络的利用率。

- 但只要网络出现拥塞或有可能出现拥塞，就必须把拥塞窗口减小一些，以减少注入到网络中的分组数，以便缓解网络出现的拥塞。

#### (2) 拥塞的判断

| 重传定时器超时       | 收到三个重复的 ACK                               |
| -------------------- | ------------------------------------------------ |
| 网络已经发生了拥塞。 | 预示网络可能会出现拥塞（实际可能还未发生拥塞）。 |

#### (3) TCP 拥塞控制算法

- 四种拥塞控制算法（ RFC 5681） ：

  - 慢开始 (slow-start)

  - 拥塞避免 (congestion avoidance)

  - 快重传 (fast retransmit)

  - 快恢复 (fast recovery)

#### (4) 慢开始 (Slow start)

- **目的**：用来确定网络的负载能力或拥塞程度。

- **算法的思路**：由小到大逐渐增大拥塞窗口数值。

- **两个变量**：

##### a. 拥塞窗口:

<u>初始拥塞窗口值：2 种设置方法</u>。

1 至 2 个最大报文段 （旧标准）

2 至 4 个最大报文段 （RFC 5681）

<u>窗口值逐渐增大</u>。

##### b. 慢开始门限

防止拥塞窗口增长过大引起网络拥塞。

- <u>拥塞窗口 cwnd 控制方法</u>：在每收到一个<u>对新的报文段的确认</u>后，可以把拥塞窗口增加最多一个 SMSS 的数值。

$$
拥塞窗口 cwnd 每次的增加量 = Min (N, SMSS)
$$

- 其中 _N_ 是原先未被确认的、但现在被刚收到的确认报文段所确认的字节数。

- 不难看出，当 _N_ < SMSS 时，拥塞窗口每次的增加量要小于 SMSS。

- 用这样的方法逐步增大发送方的拥塞窗口 cwnd，可以使分组注入到网络的速率更加合理。

#### (5) 传输轮次

- 使用慢开始算法后，每经过一个**传输轮次** (transmission round)，拥塞窗口 cwnd 就加倍。

- 一个传输轮次所经历的时间其实就是往返时间 RTT。

- “**传输轮次**”更加强调：把拥塞窗口 cwnd 所允许发送的报文段都连续发送出去，并收到了对已发送的最后一个字节的确认。

- 例如，拥塞窗口 cwnd = 4，这时的往返时间 RTT 就是发送方连续发送 4 个报文段，并收到这 4 个报文段的确认，总共经历的时间。

#### (6) 设置慢开始门限状态变量 ssthresh

- 慢开始门限 ssthresh 的用法如下：

1. 当 cwnd < ssthresh 时，使用慢开始算法。

2. 当 cwnd > ssthresh 时，停止使用慢开始算法而改用拥塞避免算法。

3. 当 cwnd = ssthresh 时，既可使用慢开始算法，也可使用拥塞避免算法。

#### (7) 拥塞避免算法

- **思路**：让拥塞窗口 cwnd <u>缓慢地增大</u>，避免出现拥塞。

- 每经过一个传输轮次，拥塞窗口 cwnd = cwnd + 1。

- 使拥塞窗口 cwnd <u>按线性规律缓慢增长</u>。

- 在拥塞避免阶段，具有 “**加法增大**” (Additive Increase) 的特点。

#### (8) 当网络出现拥塞时

- 无论在慢开始阶段还是在拥塞避免阶段，只要发送方判断网络出现拥塞（<u>重传定时器超时</u>）：

1. ssthresh = max (cwnd/2，2)

2. cwnd = 1

3. 执行慢开始算法

- **目的**：迅速减少主机发送到网络中的分组数，使得发生拥塞的路由器有足够时间把队列中积压的分组处理完毕。

#### (9) 必须强调指出

- “拥塞避免”并非指完全能够避免了拥塞。利用以上的措施要完全避免网络拥塞还是不可能的。

- “拥塞避免”是说在拥塞避免阶段把拥塞窗口控制为按线性规律增长，<u>使网络比较不容易出现拥塞</u>。

#### (10) 快重传算法

- <u>发送方只要一连收到三个重复确认</u>，就知道接收方确实没有收到报文段，因而应当<u>立即进行重传（即“快重传”）</u>，这样就不会出现超时，发送方也不就会误认为出现了网络拥塞。

- 使用快重传可以使整个网络的吞吐量提高约 20%。

> 不难看出，快重传并非取消重传计时器，而是在某些情况下可以更早地（更快地）重传丢失的报文段。

- 采用**快重传** FR (Fast Retransmission) 算法可以让发送方<u>尽早知道发生了个别报文段的丢失</u>。

- **快重传** 算法首先要求接收方不要等待自己发送数据时才进行捎带确认，而是要立即发送确认，即使收到了失序的报文段也要立即发出对已收到的报文段的重复确认。

#### (11) 快恢复算法

- 当发送端收到连续三个重复的确认时，由于发送方现在认为网络很可能没有发生拥塞，因此现在不执行慢开始算法，而是执行快恢复算法 FR (Fast Recovery) 算法：

1. 慢开始门限 ssthresh = 当前拥塞窗口 cwnd / 2 ；

2. 新拥塞窗口 cwnd = 慢开始门限 ssthresh ；

3. 开始执行拥塞避免算法，使拥塞窗口缓慢地线性增大。

#### (12) 加法增大，乘法减小 (AIMD)

- 可以看出，在拥塞避免阶段，拥塞窗口是按照线性规律增大的。这常称为“**加法增大**” AI (Additive Increase)。

- 当出现超时或 3 个重复的确认时，就要把门限值设置为当前拥塞窗口值的一半，并大大减小拥塞窗口的数值。这常称为“**乘法减小**”MD (Multiplicative Decrease)。

- 二者合在一起就是所谓的 AIMD 算法。

#### (13) 发送窗口的上限值

- 发送方的发送窗口的上限值应当取为接收方窗口 rwnd 和拥塞窗口 cwnd 这两个变量中较小的一个，即应按以下公式确定：

$$
发送窗口的上限值 = Min [rwnd, cwnd]
$$

- 当 rwnd < cwnd 时，是接收方的接收能力限制发送窗口的最大值。

- 当 cwnd < rwnd 时，则是网络的拥塞限制发送窗口的最大值。

> 也就是说，rwnd 和 cwnd 中数值较小的一个，控制了发送方发送数据的速率。

### 3. 主动队列管理 AQM

- TCP 拥塞控制和网络层采取的策略有密切联系。

- 若路由器对某些分组的处理时间特别长，那么这就可能使这些分组中的 TCP 报文段经过很长时间才能到达终点，结果引起发送方超时，对这些报文段进行重传。

- 重传会使 TCP 连接的发送端认为在网络中发生了拥塞，但实际上网络并没有发生拥塞。

- 对 TCP 拥塞控制影响最大的就是路由器的分组丢弃策略。

#### (1) “先进先出”FIFO 处理规则

- 路由器的队列通常都是按照 “**先进先出**” FIFO (First In First Out) 的规则处理到来的分组。

- 当队列已满时，以后再到达的所有分组（如果能够继续排队，这些分组都将排在队列的尾部）将都被丢弃。这就叫做**尾部丢弃策略** (tail-drop policy)。

- 路由器的尾部丢弃往往会导致一连串分组的丢失，这就使发送方出现超时重传，使 TCP 进入拥塞控制的慢开始状态，结果使 TCP 连接的发送方突然把数据的发送速率降低到很小的数值。

#### (2) 全局同步

- 更为严重的是，在网络中通常有很多的 TCP 连接，这些连接中的报文段通常是复用在网络层的 IP 数据报中传送的。

- 在这种情况下，若发生了路由器中的尾部丢弃，就可能会同时影响到很多条 TCP 连接，结果使这许多 TCP 连接在同一时间突然都进入到慢开始状态。这在 TCP 的术语中称为**全局同步** (global syncronization)。

- 全局同步使得全网的通信量突然下降了很多，而在网络恢复正常后，其通信量又突然增大很多。

#### (3) 主动队列管理 AQM

- 1998 年提出了**主动队列管理 AQM** (Active Queue Management)。

- 所谓“**主动**”就是不要等到路由器的队列长度已经达到最大值时才不得不丢弃后面到达的分组，而是在队列长度达到某个值得警惕的数值时（即当网络拥塞有了某些拥塞征兆时），就<u>主动丢弃</u>到达的分组。

- AQM 可以有不同实现方法，其中曾流行多年的就是**随机早期检测 RED** (Random Early Detection)。

#### (4) 随机早期检测 RED

- 使路由器的队列维持两个参数：队列长度最小门限 TH~min~ 和最大门限 Th~max~ 。

- RED 对每一个到达的分组都先计算平均队列长度 *L*AV 。

1. 若平均队列长度<u>小于</u>最小门限 TH~min~，则将新到达的分组放入队列进行排队。

2. 若平均队列长度<u>超过</u>最大门限 Th~max~ ，则将新到达的分组丢弃。

3. 若平均队列长度在最小门限 TH~min~ 和最大门限 Th~max~ <u>之间</u>，则按照某一**概率 _p_** 将新到达的分组丢弃。

也就是：

- 当 *L*AV < Th~min~ 时，丢弃概率 _p_ = 0。

- 当 *L*AV > Th~max~ 时，丢弃概率 _p_ = 1。

- 当 Th~min~ < *L*AV < Th~max~时，丢弃概率 0 < _p_ < 1 。

- 多年的实践证明，<u>RED 的使用效果并不太理想</u>。

- 2015 年公布的 RFC 7567 已经把 RFC 2309 列为“陈旧的”，并且不再推荐使用 RED。

- 对路由器进行主动队列管理 AQM 仍是必要的。

- <u>AQM 实际上就是对路由器中的分组排队进行智能管理，而不是简单地把队列的尾部丢弃</u>。

- 现在已经有几种不同的算法来代替旧的 RED，但都还在实验阶段。

## 九、TCP 的运输连接管理

### 1. TCP 的连接建立

- TCP 建立连接的过程叫做**握手**。

- 握手需要在客户和服务器之间交换三个 TCP 报文段。称之为**三报文握手**。

- 采用<u>三报文握手</u>主要是为了防止已失效的连接请求报文段突然又传送到了，因而产生错误。

#### (1) 运输连接的三个阶段

- TCP 是面向连接的协议。

- TCP 连接有三个阶段：

1. **连接建立**

2. **数据传送**

3. **连接释放**

- <u>TCP 连接的管理</u>就是使 TCP 连接的建立和释放都能正常地进行。

#### (2) TCP 连接建立过程中要解决的三个问题

1. 要使每一方能够确知对方的存在。

2. 要允许双方协商一些参数（如最大窗口值、是否使用窗口扩大选项和时间戳选项以及服务质量等）。

3. 能够对运输实体资源（如缓存大小、连接表中的项目等）进行分配。

#### (3) 客户——服务器方式

- TCP 连接的建立<u>采用客户服务器方式</u>。

- 主动发起连接建立的应用进程叫做**客户** (client)。

- 被动等待连接建立的应用进程叫做**服务器** (server)。

### 2. TCP 的连接释放

- TCP 连接释放过程比较复杂。

- 数据传输结束后，通信的双方都可释放连接。

- TCP 连接释放过程是<u>四报文握手</u>。

#### (1) A 必须等待 2MSL 的时间

- 第一，为了保证 A 发送的最后一个 ACK 报文段能够到达 B。

- 第二，防止 “已失效的连接请求报文段”出现在本连接中。

#### (2) 保活计时器

- 用来防止在 TCP 连接出现长时期的空闲。

- 保活计时器 通常设置为 2 小时 。若服务器过了 2 小时还没有收到客户的信息，它就发送探测报文段。若发送了 10 个探测报文段（每一个相隔 75 秒）还没有响应，就假定客户出了故障，因而就终止该连接。

### 3. TCP 的有限状态机

- 箭头旁边的字，表明引起这种变迁的原因，或表明发生状态变迁后又出现什么动作。

- 图中有三种不同的箭头。

1. 粗实线箭头表示对客户进程的正常变迁。

2. 粗虚线箭头表示对服务器进程的正常变迁。

3. 细线箭头表示异常变迁。
